{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b733f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usar Spark para procesar el dataset limpio de terremotos, realizar agregaciones escalables y generar datasets finales optimizados para análisis y visualización.\n",
    "\n",
    "\n",
    "import os   \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month  \n",
    "from pyspark import SparkContext\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"] += r\";C:\\hadoop\\bin\"\n",
    "\n",
    "## Se utiliza Spark para simular un entorno distribuido, preparado para escalar a volúmenes mayores.\n",
    "# Inicializar SparkSession\n",
    "spark = SparkSession \\\n",
    "      .builder \\\n",
    "      .appName(\"Earthquake ETL Spark\") \\\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "# Cargar dataset limpio\n",
    "input_path = '../data/processed/earthquakes_clean.csv'  \n",
    "df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "print(\"Esquema del DataFrame cargado:\")     \n",
    "df.printSchema()\n",
    "print(\"Primeras 20 filas del DataFrame cargado:\")\n",
    "df.show(20, truncate=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3042793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar datos por año y contar terremotos\n",
    "## Esta agregación permite analizar la evolución temporal de la actividad sísmica.\n",
    "\n",
    "yearly_counts = df.groupBy(col(\"year\").alias(\"year\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"year\")\n",
    " \n",
    "print(\"Conteo anual de terremotos:\")\n",
    "yearly_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f09922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# magnitud media por magnitude_category\n",
    "\n",
    "avg_mag_by_type = df.groupBy(\"magnitude_category\") \\\n",
    "    .avg(\"mag\") \\\n",
    "    .withColumnRenamed(\"avg(mag)\", \"avg_magnitude\") \\\n",
    "    .orderBy(\"magnitude_category\")\n",
    "print(\"Magnitud media por categoria de magnitud:\")\n",
    "\n",
    "avg_mag_by_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ef6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlacion entre depth y magnitud\n",
    "## Analizar la relación entre la profundidad del terremoto y su magnitud.\n",
    "correlation = df.stat.corr(\"depth\", \"mag\")\n",
    "print(f\"Correlación entre profundidad y magnitud: {correlation}\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## guardar los datasets agregados en parquet usando mode(\"overwrite\")\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \";\" + r\"C:\\hadoop\\bin\"\n",
    "\n",
    "avg_mag_by_type.write.mode(\"overwrite\").parquet(\"../data/processed/spark/avg_mag_by_type.parquet\")\n",
    "yearly_counts.write.mode(\"overwrite\").parquet(\"../data/processed/spark/yearly_counts.parquet\")\n",
    "\n",
    "print(\"Datasets agregados guardados en formato Parquet en la carpeta 'data/processed/spark'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7162ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Cargar dataset limpio\n",
    "input_path = '../data/processed/earthquakes_clean.csv'  \n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Filtrar datos para el primer trimestre de 2025\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df_q1_2025 = df[(df['time'].dt.year == 2025) & (df['time'].dt.month.isin([1, 2, 3]))]\n",
    "\n",
    "# Crear mapa centrado en la media de las coordenadas\n",
    "map_center = [df_q1_2025['latitude'].mean(), df_q1_2025['longitude'].mean()]\n",
    "m = folium.Map(location=map_center, zoom_start=2)\n",
    "\n",
    "# Agregar marcadores con clustering\n",
    "marker_cluster = MarkerCluster().add_to(m)  \n",
    "for idx, row in df_q1_2025.iterrows():\n",
    "    popup_text = f\"Place: {row['place']}<br>Magnitude: {row['mag']}\"\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=popup_text\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Guardar mapa en archivo HTML\n",
    "output_map_path = '../data/processed/earthquakes_q1_2025_map.html'  \n",
    "m.save(output_map_path)\n",
    "print(f'Mapa de terremotos del primer trimestre de 2025 guardado en: {os.path.abspath(output_map_path)}')   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
